{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMqeNnbE80S4PrZtVpuShV+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5xdhrCmrUhpw"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["In this section, the goal of class is appropriate setting the environment for data collecting from various web sites.\n","\n","This job is going with chatGPT..."],"metadata":{"id":"aiKbKcxXWpzy"}},{"cell_type":"code","source":["pip install requests beautifulsoup4"],"metadata":{"id":"IIFiNg5MXTNR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ChatGPT provide\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","\n","requests.packages.urllib3.disable_warnings()\n","response = requests.get(url, verify=False)\n","\n","\n","def crawl_pdf_links(url, keyword):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    pdf_links = []\n","\n","    for link in soup.find_all('a', href=True):\n","        href = link['href']\n","\n","        if href.endswith('.pdf') and keyword in href:\n","            pdf_links.append(href)\n","\n","    return pdf_links\n","\n","# 크롤링할 웹사이트 URL 설정\n","url = 'https://itto.int'#첫번째 예시\n","\n","# 수집할 키워드 설정\n","keyword = 'carbon'#첫번째 예시\n","\n","pdf_links = crawl_pdf_links(url, keyword)\n","\n","for link in pdf_links:\n","    print(link)\n"],"metadata":{"id":"V2BRHkQXXddP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Kw_BUzUOZ_PP"},"execution_count":null,"outputs":[]}]}